{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "# Setting the spark expectations environment variable to local, so that the spark session can be set accordingly for local testing\n",
    "os.environ[\"SPARKEXPECTATIONS_ENV\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Kafka locally to stream the spark-expectations stats to Kafka\n",
    "os.system(f\"sh ./docker_scripts/docker_kafka_start_script.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the spark session for delta lake\n",
    "spark = SparkSession.builder\\\n",
    "    .config( # type: ignore\n",
    "        \"spark.jars\",\n",
    "        \"./jars/spark-sql-kafka-0-10_2.12-3.0.0.jar,\"\n",
    "        \"./jars/kafka-clients-3.0.0.jar,\"\n",
    "        \"./jars/commons-pool2-2.8.0.jar,\"\n",
    "        \"./jars/spark-token-provider-kafka-0-10_2.12-3.0.0.jar\"\n",
    "    )\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\")\\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/hive/warehouse\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=/tmp/derby\")\\\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/ivy2\")\\\n",
    "    .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup the previous runs data if any existing        \n",
    "spark.sql(\"drop database if exists  dq_spark_local cascade\")\n",
    "os.system(\"rm -rf /tmp/hive/warehouse/dq_spark_local.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the database for the spark expectations local testing\n",
    "spark.sql(\"create database if not exists spark_catalog.dq_spark_local\")\n",
    "spark.sql(\" use spark_catalog.dq_spark_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate to expect zero tables in the database\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the rules table schema\n",
    "RULES_TABLE_SCHEMA = \"\"\" ( product_id STRING,\n",
    "    table_name STRING,\n",
    "    rule_type STRING,\n",
    "    rule STRING,\n",
    "    column_name STRING,\n",
    "    expectation STRING,\n",
    "    action_if_failed STRING,\n",
    "    tag STRING,\n",
    "    description STRING,\n",
    "    enable_for_source_dq_validation BOOLEAN, \n",
    "    enable_for_target_dq_validation BOOLEAN,\n",
    "    is_active BOOLEAN,\n",
    "    enable_error_drop_alert BOOLEAN,\n",
    "    error_drop_threshold INT )\n",
    "\"\"\"\n",
    "\n",
    "# Setup the rules\n",
    "RULES_DATA = \"\"\" \n",
    "    (\"your_product\", \"dq_spark_local.customer_order\",  \"row_dq\", \"customer_id_is_not_null\", \"customer_id\", \"customer_id is not null\",\"drop\", \"validity\", \"customer_id should not be null\", true, true,true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"sales_greater_than_two\", \"sales\", \"sales > 2\", \"drop\", \"accuracy\", \"sales value should be greater than two\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"discount_threshold\", \"discount\", \"discount*100 < 60\",\"drop\", \"validity\", \"discount should be less than 40\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"ship_mode_in_set\", \"ship_mode\", \"lower(trim(ship_mode)) in('second class', 'standard class', 'standard class')\", \"drop\", \"validity\", \"ship_mode mode belongs in the sets\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"row_dq\", \"profit_threshold\", \"profit\", \"profit>0\", \"drop\", \"validity\", \"profit threshold should be greater than 0\", true, true, true, true, 0)\n",
    "    \n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"sum_of_sales\", \"sales\", \"sum(sales)>10000\", \"ignore\", \"validity\", \"sum of sales should be greater than 10000\",  true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"sum_of_quantity\", \"quantity\", \"sum(quantity)>10000\", \"ignore\", \"validity\", \"sum of quantity should be greater than 10000\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"distinct_of_ship_mode\", \"ship_mode\", \"count(distinct ship_mode)<=3\", \"ignore\", \"validity\", \"ship_mode's should not be more than 3\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"agg_dq\", \"row_count\", \"*\", \"count(*)>=10000\", \"ignore\", \"validity\", \"count should not be greater than 10000\", true, true, true, false, 0)\n",
    "\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"product_missing_count_threshold\", \"*\", \"((select count(distinct product_id) from product) - (select count(distinct product_id) from order))>(select count(distinct product_id) from product)*0.2\", \"ignore\", \"validity\", \"row count threshold\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"product_category\", \"*\", \"(select count(distinct category) from product) < 5\", \"ignore\", \"validity\", \"distinct product category\", true, true, true, false, 0)\n",
    "    ,(\"your_product\", \"dq_spark_local.customer_order\", \"query_dq\", \"row_count_in_order\", \"*\", \"(select count(*) from order)<10000\", \"ignore\", \"accuracy\", \"count of the row in order dataset\", true, true, true, false, 0)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rules table\n",
    "spark.sql(f\" CREATE TABLE dq_spark_local.dq_rules {RULES_TABLE_SCHEMA} USING DELTA\")\n",
    "\n",
    "# insert the rules data\n",
    "spark.sql(f\" INSERT INTO dq_spark_local.dq_rules  values {RULES_DATA} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show rules\n",
    "spark.sql(\"select * from dq_spark_local.dq_rules order by rule_type\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from spark_expectations import _log\n",
    "from spark_expectations.examples.base_setup import set_up_delta\n",
    "from spark_expectations.core.expectations import (\n",
    "    SparkExpectations,\n",
    "    WrappedDataFrameWriter,\n",
    ")\n",
    "from spark_expectations.config.user_config import Constants as user_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the writer configuration for the spark expectations\n",
    "writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "\n",
    "# Setup the spark expectations object\n",
    "se: SparkExpectations = SparkExpectations(\n",
    "    product_id=\"your_product\",\n",
    "    rules_df=spark.table(\"dq_spark_local.dq_rules\"),\n",
    "    stats_table=\"dq_spark_local.dq_stats\",\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    debugger=False,\n",
    "    # stats_streaming_options={user_config.se_enable_streaming: False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the config for the spark expectations\n",
    "user_conf = {\n",
    "    user_config.se_notifications_enable_email: False,\n",
    "    user_config.se_notifications_email_smtp_host: \"mailhost.com\",\n",
    "    user_config.se_notifications_email_smtp_port: 25,\n",
    "    user_config.se_notifications_email_from: \"\",\n",
    "    user_config.se_notifications_email_to_other_mail_id: \"\",\n",
    "    user_config.se_notifications_email_subject: \"spark expectations - data quality - notifications\",\n",
    "    user_config.se_notifications_enable_slack: False,\n",
    "    user_config.se_notifications_slack_webhook_url: \"\",\n",
    "    user_config.se_notifications_on_start: True,\n",
    "    user_config.se_notifications_on_completion: True,\n",
    "    user_config.se_notifications_on_fail: True,\n",
    "    user_config.se_notifications_on_error_drop_exceeds_threshold_breach: True,\n",
    "    user_config.se_notifications_on_error_drop_threshold: 15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create product view\n",
    "_df_product: DataFrame = (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"./resources/product.csv\")\n",
    "    )\n",
    "_df_product.createOrReplaceTempView(\"product\")\n",
    "\n",
    "# create customer view\n",
    "_df_customer: DataFrame = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"./resources/customer.csv\")\n",
    ")\n",
    "\n",
    "_df_customer.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create order view\n",
    "_df_order: DataFrame = (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"./resources/order.csv\")\n",
    "    )\n",
    "_df_order.createOrReplaceTempView(\"order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of the order data\n",
    "_df_order.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the spark expectations with the config and all the required parameters in the code\n",
    "# Note the function should return a dataframe for the spark expectations to run or else it will throw an error\n",
    "@se.with_expectations(\n",
    "    target_table=\"dq_spark_local.customer_order\",\n",
    "    write_to_table=True,\n",
    "    user_conf=user_conf,\n",
    "    target_table_view=\"order\",\n",
    ")\n",
    "def run_se() -> DataFrame:\n",
    "    # _df_order: DataFrame = (\n",
    "    #     spark.read.option(\"header\", \"true\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .csv(\"./resources/order.csv\")\n",
    "    # )\n",
    "    # _df_order.createOrReplaceTempView(\"order\")\n",
    "\n",
    "    return _df_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function to build the dataframe and run the spark expectations\n",
    "run_se()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order of execution for the rules:\n",
    "\n",
    "* Source Query DQ\n",
    "* Source Agg DQ\n",
    "* Row DQ\n",
    "* Target Query DQ\n",
    "* Target Agg DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show tables\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review sample data in the stats table\n",
    "spark.sql(\"select * from dq_spark_local.dq_stats\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the schema of the stats table\n",
    "spark.sql(\"select * from dq_spark_local.dq_stats\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of the target table\n",
    "spark.sql(\"select count(*) from dq_spark_local.customer_order\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the target table\n",
    "spark.sql(\"select * from dq_spark_local.customer_order\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of the error table\n",
    "spark.sql(\"select count(*) from dq_spark_local.customer_order_error\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from dq_spark_local.customer_order_error\").show(truncate=False, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews the stats from the kafka stream\n",
    "spark.read\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"dq-sparkexpectations-stats\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"endingOffsets\", \"latest\")\\\n",
    "    .load()\\\n",
    "    .selectExpr(\"cast(value as string) as stats_records\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the rules table to disable is_active flag for the rule - \"sales_greater_than_two\"\n",
    "spark.sql(\"update dq_spark_local.dq_rules set is_active=false where rule='sales_greater_than_two'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the rules table  \n",
    "spark.sql(\"select * from dq_spark_local.dq_rules order by rule_type, rule\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the writer configuration for the spark expectations\n",
    "writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "\n",
    "# Setup the spark expectations object\n",
    "se: SparkExpectations = SparkExpectations(\n",
    "    product_id=\"your_product\",\n",
    "    rules_df=spark.table(\"dq_spark_local.dq_rules\"),\n",
    "    stats_table=\"dq_spark_local.dq_stats\",\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    debugger=False,\n",
    "    # stats_streaming_options={user_config.se_enable_streaming: False},\n",
    ")\n",
    "\n",
    "# Running for the second time to see the changes in the stats table\n",
    "@se.with_expectations(\n",
    "    target_table=\"dq_spark_local.customer_order\",\n",
    "    write_to_table=True,\n",
    "    user_conf=user_conf,\n",
    "    target_table_view=\"order\",\n",
    ")\n",
    "def run_se() -> DataFrame:\n",
    "    # _df_order: DataFrame = (\n",
    "    #     spark.read.option(\"header\", \"true\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .csv(\"./resources/order.csv\")\n",
    "    # )\n",
    "    # _df_order.createOrReplaceTempView(\"order\")\n",
    "\n",
    "    return _df_order\n",
    "\n",
    "run_se()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review sample data in the stats table\n",
    "spark.sql(\"select * from dq_spark_local.dq_stats\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_not_fail_records = spark.sql(\"select * from order where sales<=2\").count()\n",
    "print(should_not_fail_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_not_fail_records == 3869-3804:\n",
    "    print(\"Test Passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of the target table\n",
    "spark.sql(\"select count(*) from dq_spark_local.customer_order\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of the error table\n",
    "spark.sql(\"select count(*) from dq_spark_local.customer_order_error\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews the stats from the kafka stream\n",
    "spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\n",
    "    \"subscribe\", \"dq-sparkexpectations-stats\"\n",
    ").option(\"startingOffsets\", \"earliest\").option(\n",
    "    \"endingOffsets\", \"latest\"\n",
    ").load().selectExpr(\n",
    "    \"cast(value as string) as stats_records\"\n",
    ").show(\n",
    "    truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Scenario\n",
    "# Insert into dq_rules table to fail the job, have a dummy column to fail\n",
    "spark.sql(\"insert into dq_spark_local.dq_rules values ('your_product', 'dq_spark_local.customer_order', 'row_dq', 'dummy', 'dummy', 'dummy', 'drop', 'dummy', 'dummy', true, true, true, false, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the writer configuration for the spark expectations\n",
    "writer = WrappedDataFrameWriter().mode(\"append\").format(\"delta\")\n",
    "\n",
    "# Setup the spark expectations object\n",
    "se: SparkExpectations = SparkExpectations(\n",
    "    product_id=\"your_product\",\n",
    "    rules_df=spark.table(\"dq_spark_local.dq_rules\"),\n",
    "    stats_table=\"dq_spark_local.dq_stats\",\n",
    "    stats_table_writer=writer,\n",
    "    target_and_error_table_writer=writer,\n",
    "    debugger=False,\n",
    "    # stats_streaming_options={user_config.se_enable_streaming: False},\n",
    ")\n",
    "\n",
    "\n",
    "# Running for the second time to see the changes in the stats table\n",
    "@se.with_expectations(\n",
    "    target_table=\"dq_spark_local.customer_order\",\n",
    "    write_to_table=True,\n",
    "    user_conf=user_conf,\n",
    "    target_table_view=\"order\",\n",
    ")\n",
    "def run_se() -> DataFrame:\n",
    "    # _df_order: DataFrame = (\n",
    "    #     spark.read.option(\"header\", \"true\")\n",
    "    #     .option(\"inferSchema\", \"true\")\n",
    "    #     .csv(\"./resources/order.csv\")\n",
    "    # )\n",
    "    # _df_order.createOrReplaceTempView(\"order\")\n",
    "\n",
    "    return _df_order\n",
    "\n",
    "try:\n",
    "    run_se()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the stats table\n",
    "# Total Input, Error and Output records\n",
    "spark.sql(\"\"\"\n",
    "    SELECT SUM(input_count) AS total_input, \n",
    "    SUM(error_count) AS total_errors, \n",
    "    SUM(output_count) AS total_output \n",
    "    FROM  dq_spark_local.dq_stats\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT product_id, round(avg(success_percentage), 2) AS avg_success, \n",
    "    round(avg(error_percentage), 2) AS avg_error \n",
    "    FROM dq_spark_local.dq_stats group by product_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT product_id, key AS dq_status_type, value AS dq_status, COUNT(*) AS count\n",
    "FROM dq_spark_local.dq_stats \n",
    "LATERAL VIEW explode(dq_status) AS key, value\n",
    "GROUP BY product_id, key, value\n",
    "order by product_id, key, value;\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT product_id, table_name, rule.rule_type, rule.description, rule.rule, rule.failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(row_dq_res_summary) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT product_id, table_name, 'source_agg_dq' AS dq_type, rule.rule AS rule_type, rule.description, CAST(rule.failed_row_count AS INT) AS failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(source_agg_dq_results) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'final_agg_dq' AS dq_type, rule.rule AS rule_type, rule.description, CAST(rule.failed_row_count AS INT) AS failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(final_agg_dq_results) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'source_query_dq' AS dq_type, rule.rule AS rule_type, rule.description, CAST(rule.failed_row_count AS INT) AS failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(source_query_dq_results) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'final_query_dq' AS dq_type, rule.rule AS rule_type, rule.description, CAST(rule.failed_row_count AS INT) AS failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(final_query_dq_results) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'row_dq' AS dq_type, rule.rule AS rule_type, rule.description, CAST(rule.failed_row_count AS INT) AS failed_row_count\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(row_dq_res_summary) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0\n",
    "\"\"\").show(truncate=False, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select *, count(*) as failed_count from (\n",
    "    SELECT product_id, table_name, 'source_agg_dq' AS dq_type, rule.rule AS rule_type, rule.description\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(source_agg_dq_results) exploded_table AS rule\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'final_agg_dq' AS dq_type, rule.rule AS rule_type, rule.description\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(final_agg_dq_results) exploded_table AS rule\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'source_query_dq' AS dq_type, rule.rule AS rule_type, rule.description\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(source_query_dq_results) exploded_table AS rule\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'final_query_dq' AS dq_type, rule.rule AS rule_type, rule.description\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(final_query_dq_results) exploded_table AS rule\n",
    "    UNION ALL\n",
    "    SELECT product_id, table_name, 'row_dq' AS dq_type, rule.rule AS rule_type, rule.description\n",
    "    FROM dq_spark_local.dq_stats \n",
    "    LATERAL VIEW explode(row_dq_res_summary) exploded_table AS rule\n",
    "    WHERE CAST(rule.failed_row_count AS INT) > 0) group by all\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove spark session\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the docker container\n",
    "# os.system(f\"sh ./docker_scripts/docker_kafka_stop_script.sh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-expectations3105",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
